{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import datetime\nimport numpy as np             #for numerical computations like log,exp,sqrt etc\nimport pandas as pd            #for reading & storing data, pre-processing\nimport matplotlib.pylab as plt #for visualization\n#for making sure matplotlib plots are generated in Jupyter notebook itself\n%matplotlib inline             \nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.stattools import acf, pacf\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 10, 6","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/air-passengers/AirPassengers.csv\")\nprint(df.shape)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Month'] = pd.to_datetime(df[\"Month\"], infer_datetime_format=True)\nindexedDataset = df.set_index(['Month'])\nindexedDataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.xlabel(\"Date\")\nplt.ylabel(\"Number of air passengers\")\na = plt.plot(indexedDataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#window size 12 denotes 12 months, giving rolling mean at yearly level\nrolmean = indexedDataset.rolling(window=12).mean()\nrolstd = indexedDataset.rolling(window=12).std()\nprint(rolmean, rolstd)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot rolling statistics\norig = plt.plot(indexedDataset, color ='blue', label='Original')\nmean = plt.plot(rolmean, color='red', label ='Rollilng Mean')\nstd = plt.plot(rolstd, color='black', label ='Rolling Std')\nplt.legend(loc = 'best')\nplt.title('Rolling Mean & Standars Deviation')\nplt.show(block=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Perform Augmented Dickey–Fuller test:\nprint('Results of Dickey Fuller Test: ')\ndftest = adfuller(indexedDataset['#Passengers'], autolag ='AIC')\ndfoutput = pd.Series(dftest[0:4], index=['Test Statistics','p-value','#Lags Used','Number of Observaions Used'])\n\nfor key, value in dftest[4].items():\n    dfoutput['Critical Value (%s)' %key] = value\n    \nprint(dfoutput)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For a Time series to be stationary, its ADCF test should have:\n    1. p-value to be low (according to the null hypothesis)\n    2. The critical values at 1%,5%,10% confidence intervals should be as close as possible to the Test Statistics\np-value to be low (according to the null hypothesis)\nThe critical values at 1%,5%,10% confidence intervals should be as close as possible to the Test Statistics\nFrom the above ADCF test result, we see that p-value(at max can be 1.0) is very large. Also critical values are no where close to the Test Statistics. Hence, we can safely say that our Time Series at the moment is not stationary"},{"metadata":{},"cell_type":"markdown","source":"**Data Transformation to achieve Stationarity**\n\nData Transformation to achieve Stationarity \nThere are a couple of ways to achieve stationarity through data transformation like taking  log10 , loge , square, square root, cube, cube root, exponential decay, time shift and so on ...\n\nIn our notebook, lets start of with log transformations. Our objective is to remove the trend component. Hence, flatter curves( ie: paralle to x-axis) for time series and rolling mean after taking log would say that our data transformation did a good job."},{"metadata":{"trusted":true},"cell_type":"code","source":"indexedDataset_logScale = np.log(indexedDataset)\na= plt.plot(indexedDataset_logScale)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movingAverage = indexedDataset_logScale.rolling(window=12).mean()\nmovingSTD = indexedDataset_logScale.rolling(window=12).std()\na = plt.plot(indexedDataset_logScale)\na = plt.plot(movingAverage, color='red')\n# plt.plot(movingSTD, color='green')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We know from above graph that both the Time series with log scale as well as its moving average have a trend component. Thus we can apply a elementary intuition: subtraction one from the other should remove the trend component of both. Its like:\n\nlogscaleL=stationarypart(L1)+trend(LT) \nmovingavgoflogscaleA=stationarypart(A1)+trend(AT) \nresultseriesR=L−A=(L1+LT)−(A1+AT)=(L1−A1)+(LT−AT) \nSince, L & A are series & it moving avg, their trend will be more or less same, Hence\nLT-AT nearly equals to 0\n\nThus trend component will be almost removed. And we have,\n\nR=L1−A1 , our final non-trend curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"datasetLogScaleMinusMovingAverage = indexedDataset_logScale - movingAverage\ndatasetLogScaleMinusMovingAverage.head(12)\n\n# remove NAN values \ndatasetLogScaleMinusMovingAverage.dropna(inplace=True)\ndatasetLogScaleMinusMovingAverage.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test_stationarity(timeseries):\n    # datetime rolling statistics\n    movingAverage = timeseries.rolling(window=12).mean()\n    movingSTD = timeseries.rolling(window=12).std()\n    \n    # plot rolling statistics\n    orig = plt.plot(timeseries, color='blue', label='Original')\n    mean = plt.plot(movingAverage, color='red', label ='Rolling Mean')\n    std = plt.plot(movingSTD, color='black', label='Rolling Std')\n    plt.legend(loc='best')\n    plt.title(\"Rolling Mean & Standard Deviation\")\n    plt.show(block=False)\n    \n    # perform Dickey-Fuller test\n    print('Results of Dickey Fuller Test:')\n    dftest = adfuller(timeseries['#Passengers'], autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print(dfoutput)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_stationarity(datasetLogScaleMinusMovingAverage)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above graph, we observe that our intuition that \"subtracting two related series having similar trend components will make the result stationary\" is true. We find that:\n\n    1. p-value has reduced from 0.99 to 0.022.\n    2. The critical values at 1%,5%,10% confidence intervals are pretty close to the Test Statistic. Thus, from above 2 points, we can say that our given series is stationary.\nBut, in the spirit of getting higher accuracy, let us explore & try to find a better scale than our current log.\nLet us try out Exponential decay.\nFor further info, refer to my answer 12 at the top of the notebook on it."},{"metadata":{},"cell_type":"markdown","source":"**Exponential Decay Transformation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"exponentialDecayWeightedAverage = indexedDataset_logScale.ewm(\n    halflife = 12, min_periods = 0, adjust=True).mean()\nplt.plot(indexedDataset_logScale)\nplt.plot(exponentialDecayWeightedAverage, color='red')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"datasetLogScaleMinusExponentialMovingAverage = indexedDataset_logScale - exponentialDecayWeightedAverage\ntest_stationarity(datasetLogScaleMinusExponentialMovingAverage)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Time Shift Transformation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataLogDiffShifting = indexedDataset_logScale - indexedDataset_logScale.shift()\nplt.plot(dataLogDiffShifting)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataLogDiffShifting.dropna(inplace=True)\ntest_stationarity(dataLogDiffShifting)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decomposition = seasonal_decompose(indexedDataset_logScale) \n\ntrend = decomposition.trend\nseasonal = decomposition.seasonal\nresidual = decomposition.resid\n\nplt.subplot(411)\nplt.plot(indexedDataset_logScale, label='Original')\nplt.legend(loc='best')\n\nplt.subplot(412)\nplt.plot(trend, label='Trend')\nplt.legend(loc='best')\n\nplt.subplot(411)\nplt.plot(seasonal, label='Seasonality')\nplt.legend(loc='best')\n\nplt.subplot(411)\nplt.plot(residual, label='Residuals')\nplt.legend(loc='best')\n\nplt.tight_layout()\n\ndecomposedLogData = residual\ndecomposedLogData.dropna(inplace=True)\ntest_stationarity(decomposedLogData)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decomposedLogData = residual\ndecomposedLogData.dropna(inplace=True)\ntest_stationarity(decomposedLogData)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Plotting ACF & PACF**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ACF & PACF plots \n\nlag_acf = acf(dataLogDiffShifting, nlags =20)\nlag_pacf = pacf(dataLogDiffShifting, nlags=20, method='ols')\n\n# plot ACF \nplt.subplot(121)\nplt.plot(lag_acf)\nplt.axhline(y=0, linestyle=\"--\", color='gray')\nplt.axhline(y=-1.96/np.sqrt(len(dataLogDiffShifting)), linestyle=\"--\", color='gray')\nplt.axhline(y=1.96/np.sqrt(len(dataLogDiffShifting)), linestyle=\"--\", color='gray')\nplt.title(\"Autocorrelation Function\")\n\n# plot PACF\nplt.subplot(122)\nplt.plot(lag_pacf)\nplt.axhline(y=0, linestyle=\"--\", color='gray')\nplt.axhline(y=-1.96/np.sqrt(len(dataLogDiffShifting)), linestyle=\"--\", color='gray')\nplt.axhline(y=1.96/np.sqrt(len(dataLogDiffShifting)), linestyle=\"--\", color='gray')\nplt.title(\"Partial Autocorrelation Function\")\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the ACF graph, we see that curve touches y=0.0 line at x=2. Thus, from theory, Q = 2 From the PACF graph, we see that curve touches y=0.0 line at x=2. Thus, from theory, P = 2\n\nARIMA is AR + I + MA. Before, we see an ARIMA model, let us check the results of the individual AR & MA model. Note that, these models will give a value of RSS. Lower RSS values indicate a better model."},{"metadata":{},"cell_type":"markdown","source":"**Building Models**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# AR MODEL\n\nmodel = ARIMA(indexedDataset_logScale, order=(2,1,0))\nresults_AR = model.fit(disp =-1)\nplt.plot(dataLogDiffShifting)\nplt.plot(results_AR.fittedvalues, color='red')\nplt.title('RSS: %.4f'%sum((results_AR.fittedvalues - dataLogDiffShifting['#Passengers'])**2))\nprint('Plotting AR model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#MA Model\nmodel = ARIMA(indexedDataset_logScale, order=(0,1,2))\nresults_MA = model.fit(disp=-1)\nplt.plot(dataLogDiffShifting)\nplt.plot(results_MA.fittedvalues, color='red')\nplt.title('RSS: %.4f'%sum((results_MA.fittedvalues - dataLogDiffShifting['#Passengers'])**2))\nprint('Plotting MA model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# AR+I+MA = ARIMA model\nmodel = ARIMA(indexedDataset_logScale, order=(2,1,2))\nresults_ARIMA = model.fit(disp=-1)\nplt.plot(dataLogDiffShifting)\nplt.plot(results_ARIMA.fittedvalues, color='red')\nplt.title('RSS: %.4f'%sum((results_ARIMA.fittedvalues - dataLogDiffShifting['#Passengers'])**2))\nprint('Plotting ARIMA model')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With the ARIMA model built, we will now generate predictions. But, before we do any plots for predictions ,we need to reconvert the predictions back to original form. This is because, our model was built on log transformed data."},{"metadata":{},"cell_type":"markdown","source":"**Prediction & Reverse tranformations**"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_ARIMA_diff = pd.Series(results_ARIMA.fittedvalues, copy=True)\nprint(predictions_ARIMA_diff.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Convert to cumulative sum\npredictions_ARIMA_diff_cumsum = predictions_ARIMA_diff.cumsum()\nprint(predictions_ARIMA_diff_cumsum)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_ARIMA_log = pd.Series(indexedDataset_logScale['#Passengers'].iloc[0]\n                                  , index= indexedDataset_logScale.index)\npredictions_ARIMA_log = predictions_ARIMA_log.add(predictions_ARIMA_diff_cumsum, fill_value=0)\npredictions_ARIMA_log.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Inverse of log is exp.\npredictions_ARIMA = np.exp(predictions_ARIMA_log)\nplt.plot(indexedDataset)\nplt.plot(predictions_ARIMA)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"indexedDataset_logScale","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We have 144(existing data of 12 yrs in months) data points. \n#And we want to forecast for additional 120 data points or 10 yrs.\nresults_ARIMA.plot_predict(1,264)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}
